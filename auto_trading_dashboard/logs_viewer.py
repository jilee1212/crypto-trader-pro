"""
ğŸ“‹ Phase 4 Comprehensive Logs Viewer - ì¢…í•© ë¡œê·¸ ë·°ì–´

Phase 4ì˜ ê³ ê¸‰ ë¡œê·¸ ê´€ë¦¬ ë° ë¶„ì„ ì‹œìŠ¤í…œ:
- ì‹¤ì‹œê°„ ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë°
- ë‹¤ì¤‘ ë¡œê·¸ ì†ŒìŠ¤ í†µí•©
- ê³ ê¸‰ í•„í„°ë§ ë° ê²€ìƒ‰
- ë¡œê·¸ ë¶„ì„ ë° íŒ¨í„´ ê°ì§€
- ì˜¤ë¥˜ ì¶”ì  ë° ë””ë²„ê¹…
- ë¡œê·¸ ë‚´ë³´ë‚´ê¸° ë° ë°±ì—…
"""

import streamlit as st
import pandas as pd
import plotly.graph_objects as go
import plotly.express as px
from plotly.subplots import make_subplots
from datetime import datetime, timedelta
import time
import re
import json
from typing import Dict, List, Any, Optional, Tuple
import logging
from enum import Enum

# Phase 4: ê³ ê¸‰ ë¡œê·¸ ê´€ë¦¬ë¥¼ ìœ„í•œ ì¶”ê°€ import
try:
    import sys
    import os
    sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(os.path.abspath(__file__)))))
    from auto_trading.engine import AutoTradingEngine
    from utils.notifications import NotificationManager
    REAL_ENGINE_AVAILABLE = True
except ImportError:
    REAL_ENGINE_AVAILABLE = False

class LogLevel(Enum):
    """ë¡œê·¸ ë ˆë²¨"""
    DEBUG = "DEBUG"
    INFO = "INFO"
    WARNING = "WARNING"
    ERROR = "ERROR"
    CRITICAL = "CRITICAL"

class LogSource(Enum):
    """ë¡œê·¸ ì†ŒìŠ¤"""
    ENGINE = "AUTO_TRADING_ENGINE"
    RISK_MANAGER = "RISK_MANAGER"
    TRADE_EXECUTOR = "TRADE_EXECUTOR"
    SIGNAL_GENERATOR = "SIGNAL_GENERATOR"
    NOTIFICATION = "NOTIFICATION_SYSTEM"
    DATABASE = "DATABASE"
    API = "API_CONNECTOR"
    DASHBOARD = "DASHBOARD"

class AdvancedLogsViewer:
    """ğŸ“‹ Phase 4 ê³ ê¸‰ ë¡œê·¸ ë·°ì–´"""

    def __init__(self):
        self.logger = logging.getLogger(__name__)

        # ì‹¤ì œ ì—”ì§„ ì—°ë™
        if REAL_ENGINE_AVAILABLE:
            try:
                self.engine = AutoTradingEngine()
                self.real_engine = True
                self.logger.info("Phase 4: ì‹¤ì œ ì—”ì§„ ë¡œê·¸ ì‹œìŠ¤í…œê³¼ ì—°ë™ë¨")
            except Exception as e:
                self.logger.warning(f"ì—”ì§„ ë¡œê·¸ ì—°ë™ ì‹¤íŒ¨, ì‹œë®¬ë ˆì´ì…˜ ëª¨ë“œ: {e}")
                self.engine = None
                self.real_engine = False
        else:
            self.engine = None
            self.real_engine = False

        # ë¡œê·¸ ìºì‹œ ë° ì„¤ì •
        self.log_cache = []
        self.filtered_logs = []
        self.log_patterns = {}
        self.error_tracking = {}

        # ì„¸ì…˜ ìƒíƒœ ì´ˆê¸°í™”
        if 'logs_auto_refresh' not in st.session_state:
            st.session_state.logs_auto_refresh = False
        if 'logs_page_size' not in st.session_state:
            st.session_state.logs_page_size = 100

    def show_logs_viewer(self):
        """ë©”ì¸ ë¡œê·¸ ë·°ì–´ í‘œì‹œ"""
        st.title("ğŸ“‹ Phase 4 ì¢…í•© ë¡œê·¸ ë·°ì–´")

        # ë¡œê·¸ ì‹œìŠ¤í…œ ìƒíƒœ
        self._show_log_system_status()

        st.divider()

        # íƒ­ìœ¼ë¡œ êµ¬ì„±ëœ ë¡œê·¸ ì¸í„°í˜ì´ìŠ¤
        tab1, tab2, tab3, tab4, tab5 = st.tabs([
            "ğŸ“Š ì‹¤ì‹œê°„ ë¡œê·¸",
            "ğŸ” ë¡œê·¸ ê²€ìƒ‰",
            "ğŸ“ˆ ë¡œê·¸ ë¶„ì„",
            "ğŸš¨ ì˜¤ë¥˜ ì¶”ì ",
            "âš™ï¸ ë¡œê·¸ ê´€ë¦¬"
        ])

        with tab1:
            self._show_realtime_logs()

        with tab2:
            self._show_log_search()

        with tab3:
            self._show_log_analytics()

        with tab4:
            self._show_error_tracking()

        with tab5:
            self._show_log_management()

    def _show_log_system_status(self):
        """ë¡œê·¸ ì‹œìŠ¤í…œ ìƒíƒœ í‘œì‹œ"""
        if self.real_engine:
            st.success("ğŸŸ¢ ì‹¤ì œ ë¡œê·¸ ì‹œìŠ¤í…œ ì—°ê²°ë¨")
        else:
            st.warning("ğŸŸ¡ ì‹œë®¬ë ˆì´ì…˜ ë¡œê·¸ ëª¨ë“œ")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            total_logs = len(self.log_cache)
            st.metric("ì´ ë¡œê·¸ ìˆ˜", f"{total_logs:,}")

        with col2:
            if self.log_cache:
                latest_log_time = max(log['timestamp'] for log in self.log_cache)
                time_diff = (datetime.now() - latest_log_time).seconds
                st.metric("ìµœì‹  ë¡œê·¸", f"{time_diff}ì´ˆ ì „")
            else:
                st.metric("ìµœì‹  ë¡œê·¸", "ì—†ìŒ")

        with col3:
            error_count = len([log for log in self.log_cache if log.get('level') in ['ERROR', 'CRITICAL']])
            st.metric("ì˜¤ë¥˜ ë¡œê·¸", f"{error_count}")

        with col4:
            active_sources = len(set(log.get('source', 'UNKNOWN') for log in self.log_cache))
            st.metric("í™œì„± ì†ŒìŠ¤", f"{active_sources}")

    def _show_realtime_logs(self):
        """ì‹¤ì‹œê°„ ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë°"""
        st.markdown("### ğŸ“Š ì‹¤ì‹œê°„ ë¡œê·¸ ìŠ¤íŠ¸ë¦¬ë°")

        # ì‹¤ì‹œê°„ ì œì–´
        col1, col2, col3 = st.columns(3)

        with col1:
            auto_refresh = st.checkbox(
                "ìë™ ìƒˆë¡œê³ ì¹¨ (5ì´ˆ)",
                value=st.session_state.logs_auto_refresh,
                key="logs_auto_refresh_toggle"
            )
            st.session_state.logs_auto_refresh = auto_refresh

        with col2:
            page_size = st.selectbox(
                "í˜ì´ì§€ í¬ê¸°",
                [50, 100, 200, 500],
                index=1,
                key="logs_page_size_select"
            )
            st.session_state.logs_page_size = page_size

        with col3:
            if st.button("ğŸ”„ ë¡œê·¸ ìƒˆë¡œê³ ì¹¨", key="logs_manual_refresh"):
                self._refresh_logs()

        # ìë™ ìƒˆë¡œê³ ì¹¨
        if auto_refresh:
            time.sleep(5)
            self._refresh_logs()
            st.rerun()

        # ë¹ ë¥¸ í•„í„°
        st.markdown("#### ğŸ” ë¹ ë¥¸ í•„í„°")

        col1, col2, col3 = st.columns(3)

        with col1:
            level_filter = st.multiselect(
                "ë¡œê·¸ ë ˆë²¨",
                [level.value for level in LogLevel],
                default=[LogLevel.INFO.value, LogLevel.WARNING.value, LogLevel.ERROR.value],
                key="realtime_level_filter"
            )

        with col2:
            source_filter = st.multiselect(
                "ë¡œê·¸ ì†ŒìŠ¤",
                [source.value for source in LogSource],
                default=[],
                key="realtime_source_filter"
            )

        with col3:
            keyword_filter = st.text_input(
                "í‚¤ì›Œë“œ ê²€ìƒ‰",
                placeholder="ê²€ìƒ‰í•  í‚¤ì›Œë“œ ì…ë ¥...",
                key="realtime_keyword_filter"
            )

        # ë¡œê·¸ ë°ì´í„° ë¡œë“œ ë° í•„í„°ë§
        logs = self._get_filtered_logs(level_filter, source_filter, keyword_filter)

        # ë¡œê·¸ í…Œì´ë¸” í‘œì‹œ
        if logs:
            st.markdown(f"#### ğŸ“‹ ë¡œê·¸ ëª©ë¡ (ìµœê·¼ {len(logs)}ê°œ)")

            # ë¡œê·¸ ë ˆë²¨ë³„ ìƒ‰ìƒ ë§¤í•‘
            level_colors = {
                'DEBUG': 'ğŸ”µ',
                'INFO': 'ğŸŸ¢',
                'WARNING': 'ğŸŸ¡',
                'ERROR': 'ğŸ”´',
                'CRITICAL': 'ğŸŸ£'
            }

            # ë¡œê·¸ í•­ëª©ë“¤ì„ expanderë¡œ í‘œì‹œ
            for i, log in enumerate(logs[:page_size]):
                level_icon = level_colors.get(log.get('level', 'INFO'), 'âšª')
                timestamp = log.get('timestamp', datetime.now()).strftime("%H:%M:%S")
                source = log.get('source', 'UNKNOWN')
                message = log.get('message', 'No message')

                with st.expander(f"{level_icon} {timestamp} [{source}] {message[:100]}...", expanded=False):
                    col1, col2 = st.columns([3, 1])

                    with col1:
                        st.markdown(f"**ë©”ì‹œì§€:** {message}")
                        if 'data' in log and log['data']:
                            st.markdown("**ì¶”ê°€ ë°ì´í„°:**")
                            st.json(log['data'])

                    with col2:
                        st.markdown(f"**ë ˆë²¨:** {log.get('level', 'INFO')}")
                        st.markdown(f"**ì†ŒìŠ¤:** {source}")
                        st.markdown(f"**ì‹œê°„:** {log.get('timestamp', datetime.now()).strftime('%Y-%m-%d %H:%M:%S')}")

        else:
            st.info("í‘œì‹œí•  ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤")

    def _show_log_search(self):
        """ê³ ê¸‰ ë¡œê·¸ ê²€ìƒ‰"""
        st.markdown("### ğŸ” ê³ ê¸‰ ë¡œê·¸ ê²€ìƒ‰")

        # ê²€ìƒ‰ ì„¤ì •
        col1, col2 = st.columns([2, 1])

        with col1:
            st.markdown("#### ê²€ìƒ‰ ì¡°ê±´")

            # ì‹œê°„ ë²”ìœ„
            time_range = st.selectbox(
                "ì‹œê°„ ë²”ìœ„",
                ["ìµœê·¼ 1ì‹œê°„", "ìµœê·¼ 6ì‹œê°„", "ìµœê·¼ 24ì‹œê°„", "ìµœê·¼ 7ì¼", "ì‚¬ìš©ì ì •ì˜"],
                index=2,
                key="search_time_range"
            )

            if time_range == "ì‚¬ìš©ì ì •ì˜":
                col_start, col_end = st.columns(2)
                with col_start:
                    start_date = st.date_input("ì‹œì‘ ë‚ ì§œ", key="search_start_date")
                    start_time = st.time_input("ì‹œì‘ ì‹œê°„", key="search_start_time")
                with col_end:
                    end_date = st.date_input("ì¢…ë£Œ ë‚ ì§œ", key="search_end_date")
                    end_time = st.time_input("ì¢…ë£Œ ì‹œê°„", key="search_end_time")

            # ê³ ê¸‰ í•„í„°
            search_levels = st.multiselect(
                "ë¡œê·¸ ë ˆë²¨",
                [level.value for level in LogLevel],
                default=[],
                key="search_level_filter"
            )

            search_sources = st.multiselect(
                "ë¡œê·¸ ì†ŒìŠ¤",
                [source.value for source in LogSource],
                default=[],
                key="search_source_filter"
            )

            # í…ìŠ¤íŠ¸ ê²€ìƒ‰
            search_text = st.text_area(
                "ê²€ìƒ‰ í…ìŠ¤íŠ¸ (ì •ê·œì‹ ì§€ì›)",
                placeholder="ê²€ìƒ‰í•  í…ìŠ¤íŠ¸ ë˜ëŠ” ì •ê·œì‹ íŒ¨í„´...",
                key="search_text_input"
            )

            regex_mode = st.checkbox("ì •ê·œì‹ ëª¨ë“œ", key="search_regex_mode")

        with col2:
            st.markdown("#### ê²€ìƒ‰ ì˜µì…˜")

            case_sensitive = st.checkbox("ëŒ€ì†Œë¬¸ì êµ¬ë¶„", key="search_case_sensitive")
            include_data = st.checkbox("ì¶”ê°€ ë°ì´í„° í¬í•¨", value=True, key="search_include_data")
            max_results = st.number_input(
                "ìµœëŒ€ ê²°ê³¼ ìˆ˜",
                min_value=10,
                max_value=10000,
                value=1000,
                key="search_max_results"
            )

        # ê²€ìƒ‰ ì‹¤í–‰
        if st.button("ğŸ” ê²€ìƒ‰ ì‹¤í–‰", type="primary", use_container_width=True, key="execute_search"):
            with st.spinner("ë¡œê·¸ ê²€ìƒ‰ ì¤‘..."):
                search_results = self._execute_advanced_search(
                    time_range, search_levels, search_sources, search_text,
                    regex_mode, case_sensitive, include_data, max_results
                )

                if search_results:
                    st.success(f"âœ… {len(search_results)}ê°œì˜ ë¡œê·¸ë¥¼ ì°¾ì•˜ìŠµë‹ˆë‹¤")

                    # ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½
                    self._show_search_summary(search_results)

                    # ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ
                    self._show_search_results(search_results)

                else:
                    st.warning("ê²€ìƒ‰ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤")

    def _show_log_analytics(self):
        """ë¡œê·¸ ë¶„ì„ ë° í†µê³„"""
        st.markdown("### ğŸ“ˆ ë¡œê·¸ ë¶„ì„ ë° í†µê³„")

        if not self.log_cache:
            st.info("ë¶„ì„í•  ë¡œê·¸ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤")
            return

        # ë¶„ì„ ê¸°ê°„ ì„ íƒ
        analysis_period = st.selectbox(
            "ë¶„ì„ ê¸°ê°„",
            ["ìµœê·¼ 1ì‹œê°„", "ìµœê·¼ 6ì‹œê°„", "ìµœê·¼ 24ì‹œê°„", "ìµœê·¼ 7ì¼"],
            index=2,
            key="analytics_period"
        )

        # ë¡œê·¸ í†µê³„ ê³„ì‚°
        stats = self._calculate_log_statistics(analysis_period)

        # í†µê³„ ì¹´ë“œ
        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("ì´ ë¡œê·¸ ìˆ˜", f"{stats['total_logs']:,}")

        with col2:
            st.metric("ë¡œê·¸ ì†ë„", f"{stats['logs_per_minute']:.1f}/ë¶„")

        with col3:
            st.metric("ì˜¤ë¥˜ìœ¨", f"{stats['error_rate']:.1f}%")

        with col4:
            st.metric("í™œì„± ì†ŒìŠ¤", f"{stats['active_sources']}")

        # ì°¨íŠ¸ë“¤
        col1, col2 = st.columns(2)

        with col1:
            # ë¡œê·¸ ë ˆë²¨ë³„ ë¶„í¬
            level_counts = stats['level_distribution']
            if level_counts:
                fig_pie = px.pie(
                    values=list(level_counts.values()),
                    names=list(level_counts.keys()),
                    title="ë¡œê·¸ ë ˆë²¨ë³„ ë¶„í¬"
                )
                st.plotly_chart(fig_pie, use_container_width=True)

        with col2:
            # ì†ŒìŠ¤ë³„ ë¶„í¬
            source_counts = stats['source_distribution']
            if source_counts:
                fig_bar = px.bar(
                    x=list(source_counts.keys()),
                    y=list(source_counts.values()),
                    title="ì†ŒìŠ¤ë³„ ë¡œê·¸ ë¶„í¬"
                )
                fig_bar.update_xaxes(tickangle=45)
                st.plotly_chart(fig_bar, use_container_width=True)

        # ì‹œê°„ë³„ ë¡œê·¸ ì¶”ì´
        st.markdown("#### ğŸ“Š ì‹œê°„ë³„ ë¡œê·¸ ì¶”ì´")

        hourly_stats = stats['hourly_distribution']
        if hourly_stats:
            fig_line = go.Figure()

            for level in LogLevel:
                level_data = [hourly_stats.get(hour, {}).get(level.value, 0) for hour in sorted(hourly_stats.keys())]
                fig_line.add_trace(go.Scatter(
                    x=list(sorted(hourly_stats.keys())),
                    y=level_data,
                    mode='lines+markers',
                    name=level.value
                ))

            fig_line.update_layout(
                title="ì‹œê°„ë³„ ë¡œê·¸ ë ˆë²¨ ì¶”ì´",
                xaxis_title="ì‹œê°„",
                yaxis_title="ë¡œê·¸ ìˆ˜",
                height=400
            )

            st.plotly_chart(fig_line, use_container_width=True)

        # íŒ¨í„´ ë¶„ì„
        st.markdown("#### ğŸ” ë¡œê·¸ íŒ¨í„´ ë¶„ì„")

        patterns = self._analyze_log_patterns()
        if patterns:
            for pattern_name, pattern_info in patterns.items():
                with st.expander(f"íŒ¨í„´: {pattern_name} ({pattern_info['count']}íšŒ ë°œê²¬)"):
                    st.write(f"**ì„¤ëª…:** {pattern_info['description']}")
                    st.write(f"**ë¹ˆë„:** {pattern_info['frequency']}")
                    if pattern_info['examples']:
                        st.write("**ì˜ˆì‹œ:**")
                        for example in pattern_info['examples'][:3]:
                            st.code(example)

    def _show_error_tracking(self):
        """ì˜¤ë¥˜ ì¶”ì  ë° ë””ë²„ê¹…"""
        st.markdown("### ğŸš¨ ì˜¤ë¥˜ ì¶”ì  ë° ë””ë²„ê¹…")

        # ì˜¤ë¥˜ í†µê³„
        error_logs = [log for log in self.log_cache if log.get('level') in ['ERROR', 'CRITICAL']]

        if not error_logs:
            st.success("âœ… í˜„ì¬ ì¶”ì ëœ ì˜¤ë¥˜ê°€ ì—†ìŠµë‹ˆë‹¤")
            return

        col1, col2, col3 = st.columns(3)

        with col1:
            st.metric("ì´ ì˜¤ë¥˜ ìˆ˜", len(error_logs))

        with col2:
            recent_errors = len([log for log in error_logs if (datetime.now() - log.get('timestamp', datetime.now())).seconds < 3600])
            st.metric("ìµœê·¼ 1ì‹œê°„ ì˜¤ë¥˜", recent_errors)

        with col3:
            critical_errors = len([log for log in error_logs if log.get('level') == 'CRITICAL'])
            st.metric("ì‹¬ê°í•œ ì˜¤ë¥˜", critical_errors)

        # ì˜¤ë¥˜ ë¶„ë¥˜
        st.markdown("#### ğŸ“Š ì˜¤ë¥˜ ë¶„ë¥˜")

        error_analysis = self._analyze_errors(error_logs)

        # ì˜¤ë¥˜ ìœ í˜•ë³„ ì°¨íŠ¸
        if error_analysis['error_types']:
            col1, col2 = st.columns(2)

            with col1:
                fig_pie = px.pie(
                    values=list(error_analysis['error_types'].values()),
                    names=list(error_analysis['error_types'].keys()),
                    title="ì˜¤ë¥˜ ìœ í˜•ë³„ ë¶„í¬"
                )
                st.plotly_chart(fig_pie, use_container_width=True)

            with col2:
                fig_bar = px.bar(
                    x=list(error_analysis['error_sources'].keys()),
                    y=list(error_analysis['error_sources'].values()),
                    title="ì†ŒìŠ¤ë³„ ì˜¤ë¥˜ ë¶„í¬"
                )
                st.plotly_chart(fig_bar, use_container_width=True)

        # ìµœê·¼ ì˜¤ë¥˜ ëª©ë¡
        st.markdown("#### ğŸ”´ ìµœê·¼ ì˜¤ë¥˜ ëª©ë¡")

        for i, error_log in enumerate(error_logs[-10:]):  # ìµœê·¼ 10ê°œ
            level_icon = "ğŸ”´" if error_log.get('level') == 'ERROR' else "ğŸŸ£"
            timestamp = error_log.get('timestamp', datetime.now()).strftime("%m-%d %H:%M:%S")
            source = error_log.get('source', 'UNKNOWN')
            message = error_log.get('message', 'No message')

            with st.expander(f"{level_icon} {timestamp} [{source}] {message[:80]}...", expanded=False):
                st.markdown(f"**ë ˆë²¨:** {error_log.get('level', 'ERROR')}")
                st.markdown(f"**ë©”ì‹œì§€:** {message}")
                st.markdown(f"**ì†ŒìŠ¤:** {source}")
                st.markdown(f"**ì‹œê°„:** {error_log.get('timestamp', datetime.now()).strftime('%Y-%m-%d %H:%M:%S')}")

                if 'data' in error_log and error_log['data']:
                    st.markdown("**ì˜¤ë¥˜ ë°ì´í„°:**")
                    st.json(error_log['data'])

                # í•´ê²° ë°©ë²• ì œì•ˆ
                suggestions = self._get_error_suggestions(error_log)
                if suggestions:
                    st.markdown("**ğŸ’¡ í•´ê²° ë°©ë²• ì œì•ˆ:**")
                    for suggestion in suggestions:
                        st.info(f"â€¢ {suggestion}")

    def _show_log_management(self):
        """ë¡œê·¸ ê´€ë¦¬ ë° ì„¤ì •"""
        st.markdown("### âš™ï¸ ë¡œê·¸ ê´€ë¦¬ ë° ì„¤ì •")

        # ë¡œê·¸ ì„¤ì •
        st.markdown("#### ğŸ”§ ë¡œê·¸ ì„¤ì •")

        col1, col2 = st.columns(2)

        with col1:
            log_level = st.selectbox(
                "ìµœì†Œ ë¡œê·¸ ë ˆë²¨",
                [level.value for level in LogLevel],
                index=1,  # INFO
                key="log_level_setting"
            )

            max_log_size = st.number_input(
                "ìµœëŒ€ ë¡œê·¸ ìºì‹œ í¬ê¸°",
                min_value=1000,
                max_value=100000,
                value=10000,
                step=1000,
                key="max_log_size"
            )

            log_retention_days = st.number_input(
                "ë¡œê·¸ ë³´ì¡´ ê¸°ê°„ (ì¼)",
                min_value=1,
                max_value=365,
                value=30,
                key="log_retention_days"
            )

        with col2:
            enable_real_time = st.checkbox("ì‹¤ì‹œê°„ ë¡œê·¸ ìˆ˜ì§‘", value=True, key="enable_real_time_logs")
            enable_file_logging = st.checkbox("íŒŒì¼ ë¡œê¹…", value=True, key="enable_file_logging")
            enable_error_alerts = st.checkbox("ì˜¤ë¥˜ ì•Œë¦¼", value=True, key="enable_error_alerts")

        # ë¡œê·¸ ë‚´ë³´ë‚´ê¸°
        st.markdown("#### ğŸ“¤ ë¡œê·¸ ë‚´ë³´ë‚´ê¸°")

        export_format = st.selectbox(
            "ë‚´ë³´ë‚´ê¸° í˜•ì‹",
            ["CSV", "JSON", "TXT"],
            key="export_format"
        )

        export_range = st.selectbox(
            "ë‚´ë³´ë‚´ê¸° ë²”ìœ„",
            ["ì „ì²´", "ìµœê·¼ 24ì‹œê°„", "ìµœê·¼ 7ì¼", "ì˜¤ë¥˜ë§Œ"],
            key="export_range"
        )

        if st.button("ğŸ“¥ ë¡œê·¸ ë‚´ë³´ë‚´ê¸°", key="export_logs"):
            exported_data = self._export_logs(export_format, export_range)
            if exported_data:
                st.success("âœ… ë¡œê·¸ê°€ ì„±ê³µì ìœ¼ë¡œ ë‚´ë³´ë‚´ì¡ŒìŠµë‹ˆë‹¤")
                st.download_button(
                    label=f"ğŸ’¾ {export_format} íŒŒì¼ ë‹¤ìš´ë¡œë“œ",
                    data=exported_data,
                    file_name=f"logs_{datetime.now().strftime('%Y%m%d_%H%M%S')}.{export_format.lower()}",
                    key="download_logs"
                )

        # ë¡œê·¸ ì •ë¦¬
        st.markdown("#### ğŸ§¹ ë¡œê·¸ ì •ë¦¬")

        col1, col2, col3 = st.columns(3)

        with col1:
            if st.button("ğŸ—‘ï¸ ì˜¤ë˜ëœ ë¡œê·¸ ì‚­ì œ", key="cleanup_old_logs"):
                cleaned_count = self._cleanup_old_logs()
                st.success(f"âœ… {cleaned_count}ê°œì˜ ì˜¤ë˜ëœ ë¡œê·¸ë¥¼ ì‚­ì œí–ˆìŠµë‹ˆë‹¤")

        with col2:
            if st.button("ğŸ”„ ë¡œê·¸ ìºì‹œ ì´ˆê¸°í™”", key="clear_log_cache"):
                self.log_cache = []
                st.success("âœ… ë¡œê·¸ ìºì‹œê°€ ì´ˆê¸°í™”ë˜ì—ˆìŠµë‹ˆë‹¤")

        with col3:
            if st.button("ğŸ“Š ë¡œê·¸ í†µê³„ ì¬ê³„ì‚°", key="recalculate_stats"):
                self._recalculate_log_statistics()
                st.success("âœ… ë¡œê·¸ í†µê³„ê°€ ì¬ê³„ì‚°ë˜ì—ˆìŠµë‹ˆë‹¤")

    # í—¬í¼ ë©”ì„œë“œë“¤

    def _refresh_logs(self):
        """ë¡œê·¸ ìƒˆë¡œê³ ì¹¨"""
        try:
            if self.real_engine and self.engine:
                # ì‹¤ì œ ì—”ì§„ì—ì„œ ë¡œê·¸ ê°€ì ¸ì˜¤ê¸°
                pass
            else:
                # ì‹œë®¬ë ˆì´ì…˜ ë¡œê·¸ ìƒì„±
                self._generate_simulated_logs()

        except Exception as e:
            self.logger.error(f"ë¡œê·¸ ìƒˆë¡œê³ ì¹¨ ì‹¤íŒ¨: {e}")

    def _generate_simulated_logs(self):
        """ì‹œë®¬ë ˆì´ì…˜ ë¡œê·¸ ìƒì„±"""
        import random

        # ì‹œë®¬ë ˆì´ì…˜ ë¡œê·¸ ë©”ì‹œì§€ë“¤
        log_templates = [
            ("INFO", "ENGINE", "ìë™ë§¤ë§¤ ì‹œìŠ¤í…œ ì •ìƒ ì‘ë™ ì¤‘"),
            ("INFO", "TRADE_EXECUTOR", "ê±°ë˜ ì‹ í˜¸ ì²˜ë¦¬ ì™„ë£Œ: {symbol}"),
            ("WARNING", "RISK_MANAGER", "ì¼ì¼ ì†ì‹¤ í•œë„ {pct}% ë„ë‹¬"),
            ("ERROR", "API_CONNECTOR", "API ì—°ê²° ì¼ì‹œì  ì‹¤íŒ¨"),
            ("DEBUG", "SIGNAL_GENERATOR", "ê¸°ìˆ ì  ì§€í‘œ ê³„ì‚° ì™„ë£Œ"),
            ("INFO", "NOTIFICATION", "ì•Œë¦¼ ë°œì†¡ ì™„ë£Œ: {count}ê±´"),
            ("CRITICAL", "DATABASE", "ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨"),
            ("INFO", "POSITION_MANAGER", "í¬ì§€ì…˜ ì—…ë°ì´íŠ¸: {positions}ê°œ í™œì„±"),
        ]

        # ìƒˆë¡œìš´ ë¡œê·¸ 5-15ê°œ ìƒì„±
        new_log_count = random.randint(5, 15)

        for _ in range(new_log_count):
            template = random.choice(log_templates)
            level, source, message_template = template

            # ë©”ì‹œì§€ í¬ë§·íŒ…
            message = message_template.format(
                symbol=random.choice(['BTC/USDT', 'ETH/USDT', 'BNB/USDT']),
                pct=random.randint(60, 95),
                count=random.randint(1, 10),
                positions=random.randint(0, 5)
            )

            # ë¡œê·¸ ì—”íŠ¸ë¦¬ ìƒì„±
            log_entry = {
                'timestamp': datetime.now() - timedelta(seconds=random.randint(0, 3600)),
                'level': level,
                'source': source,
                'message': message,
                'data': {'simulation': True} if random.random() < 0.3 else {}
            }

            self.log_cache.append(log_entry)

        # ë¡œê·¸ ìºì‹œ í¬ê¸° ì œí•œ
        if len(self.log_cache) > 10000:
            self.log_cache = self.log_cache[-10000:]

        # ì‹œê°„ìˆœ ì •ë ¬
        self.log_cache.sort(key=lambda x: x['timestamp'], reverse=True)

    def _get_filtered_logs(self, level_filter: List[str], source_filter: List[str], keyword_filter: str) -> List[Dict]:
        """í•„í„°ë§ëœ ë¡œê·¸ ë°˜í™˜"""
        if not self.log_cache:
            self._refresh_logs()

        filtered = self.log_cache.copy()

        # ë ˆë²¨ í•„í„°
        if level_filter:
            filtered = [log for log in filtered if log.get('level') in level_filter]

        # ì†ŒìŠ¤ í•„í„°
        if source_filter:
            filtered = [log for log in filtered if log.get('source') in source_filter]

        # í‚¤ì›Œë“œ í•„í„°
        if keyword_filter:
            keyword_lower = keyword_filter.lower()
            filtered = [
                log for log in filtered
                if keyword_lower in log.get('message', '').lower()
            ]

        return filtered

    def _execute_advanced_search(self, time_range: str, levels: List[str], sources: List[str],
                                search_text: str, regex_mode: bool, case_sensitive: bool,
                                include_data: bool, max_results: int) -> List[Dict]:
        """ê³ ê¸‰ ê²€ìƒ‰ ì‹¤í–‰"""
        # ì‹œë®¬ë ˆì´ì…˜ ê²€ìƒ‰ ê²°ê³¼
        results = self.log_cache.copy()

        # ì‹œê°„ ë²”ìœ„ í•„í„°ë§
        now = datetime.now()
        if time_range == "ìµœê·¼ 1ì‹œê°„":
            cutoff = now - timedelta(hours=1)
        elif time_range == "ìµœê·¼ 6ì‹œê°„":
            cutoff = now - timedelta(hours=6)
        elif time_range == "ìµœê·¼ 24ì‹œê°„":
            cutoff = now - timedelta(hours=24)
        elif time_range == "ìµœê·¼ 7ì¼":
            cutoff = now - timedelta(days=7)
        else:
            cutoff = now - timedelta(days=30)  # ê¸°ë³¸ê°’

        results = [log for log in results if log.get('timestamp', now) >= cutoff]

        # ë ˆë²¨ í•„í„°
        if levels:
            results = [log for log in results if log.get('level') in levels]

        # ì†ŒìŠ¤ í•„í„°
        if sources:
            results = [log for log in results if log.get('source') in sources]

        # í…ìŠ¤íŠ¸ ê²€ìƒ‰
        if search_text:
            if regex_mode:
                try:
                    pattern = re.compile(search_text, 0 if case_sensitive else re.IGNORECASE)
                    results = [
                        log for log in results
                        if pattern.search(log.get('message', ''))
                    ]
                except re.error:
                    st.error("ì •ê·œì‹ íŒ¨í„´ì´ ìœ íš¨í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤")
                    return []
            else:
                search_lower = search_text if case_sensitive else search_text.lower()
                results = [
                    log for log in results
                    if search_lower in (log.get('message', '') if case_sensitive else log.get('message', '').lower())
                ]

        return results[:max_results]

    def _show_search_summary(self, results: List[Dict]):
        """ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½"""
        st.markdown("#### ğŸ“Š ê²€ìƒ‰ ê²°ê³¼ ìš”ì•½")

        col1, col2, col3, col4 = st.columns(4)

        with col1:
            st.metric("ì´ ê²°ê³¼", len(results))

        with col2:
            error_count = len([r for r in results if r.get('level') in ['ERROR', 'CRITICAL']])
            st.metric("ì˜¤ë¥˜ ë¡œê·¸", error_count)

        with col3:
            if results:
                latest = max(r.get('timestamp', datetime.now()) for r in results)
                oldest = min(r.get('timestamp', datetime.now()) for r in results)
                duration = (latest - oldest).total_seconds() / 3600
                st.metric("ì‹œê°„ ë²”ìœ„", f"{duration:.1f}ì‹œê°„")

        with col4:
            sources = len(set(r.get('source', 'UNKNOWN') for r in results))
            st.metric("ì†ŒìŠ¤ ìˆ˜", sources)

    def _show_search_results(self, results: List[Dict]):
        """ê²€ìƒ‰ ê²°ê³¼ í‘œì‹œ"""
        st.markdown("#### ğŸ“‹ ê²€ìƒ‰ ê²°ê³¼")

        for result in results[:100]:  # ìµœëŒ€ 100ê°œ í‘œì‹œ
            level_icon = {'DEBUG': 'ğŸ”µ', 'INFO': 'ğŸŸ¢', 'WARNING': 'ğŸŸ¡', 'ERROR': 'ğŸ”´', 'CRITICAL': 'ğŸŸ£'}.get(result.get('level'), 'âšª')
            timestamp = result.get('timestamp', datetime.now()).strftime("%m-%d %H:%M:%S")
            message = result.get('message', 'No message')

            st.text(f"{level_icon} {timestamp} [{result.get('source', 'UNKNOWN')}] {message}")

    def _calculate_log_statistics(self, period: str) -> Dict[str, Any]:
        """ë¡œê·¸ í†µê³„ ê³„ì‚°"""
        # ì‹œë®¬ë ˆì´ì…˜ í†µê³„
        return {
            'total_logs': len(self.log_cache),
            'logs_per_minute': len(self.log_cache) / 60 if self.log_cache else 0,
            'error_rate': 15.2,
            'active_sources': 8,
            'level_distribution': {
                'DEBUG': 45,
                'INFO': 120,
                'WARNING': 25,
                'ERROR': 8,
                'CRITICAL': 2
            },
            'source_distribution': {
                'ENGINE': 50,
                'TRADE_EXECUTOR': 35,
                'RISK_MANAGER': 20,
                'API_CONNECTOR': 15,
                'NOTIFICATION': 10
            },
            'hourly_distribution': {
                f"{i:02d}:00": {
                    'INFO': random.randint(5, 25),
                    'WARNING': random.randint(0, 5),
                    'ERROR': random.randint(0, 3)
                } for i in range(24)
            }
        }

    def _analyze_log_patterns(self) -> Dict[str, Any]:
        """ë¡œê·¸ íŒ¨í„´ ë¶„ì„"""
        return {
            'repeated_errors': {
                'count': 15,
                'description': 'API ì—°ê²° ì˜¤ë¥˜ê°€ ë°˜ë³µì ìœ¼ë¡œ ë°œìƒ',
                'frequency': '10ë¶„ë§ˆë‹¤',
                'examples': ['API connection timeout', 'Connection reset by peer']
            },
            'trading_cycles': {
                'count': 8,
                'description': 'ì •ìƒì ì¸ ê±°ë˜ ì‚¬ì´í´ íŒ¨í„´',
                'frequency': '30ë¶„ë§ˆë‹¤',
                'examples': ['Signal generated', 'Trade executed', 'Position updated']
            }
        }

    def _analyze_errors(self, error_logs: List[Dict]) -> Dict[str, Any]:
        """ì˜¤ë¥˜ ë¶„ì„"""
        error_types = {}
        error_sources = {}

        for error in error_logs:
            # ì˜¤ë¥˜ ìœ í˜• ë¶„ë¥˜
            message = error.get('message', '')
            if 'connection' in message.lower():
                error_types['Connection Error'] = error_types.get('Connection Error', 0) + 1
            elif 'timeout' in message.lower():
                error_types['Timeout Error'] = error_types.get('Timeout Error', 0) + 1
            else:
                error_types['Other Error'] = error_types.get('Other Error', 0) + 1

            # ì†ŒìŠ¤ë³„ ë¶„ë¥˜
            source = error.get('source', 'UNKNOWN')
            error_sources[source] = error_sources.get(source, 0) + 1

        return {
            'error_types': error_types,
            'error_sources': error_sources
        }

    def _get_error_suggestions(self, error_log: Dict[str, Any]) -> List[str]:
        """ì˜¤ë¥˜ í•´ê²° ë°©ë²• ì œì•ˆ"""
        message = error_log.get('message', '').lower()
        suggestions = []

        if 'connection' in message:
            suggestions.append("ë„¤íŠ¸ì›Œí¬ ì—°ê²° ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”")
            suggestions.append("API í‚¤ ì„¤ì •ì„ ì ê²€í•˜ì„¸ìš”")

        if 'timeout' in message:
            suggestions.append("ìš”ì²­ íƒ€ì„ì•„ì›ƒ ì„¤ì •ì„ ëŠ˜ë ¤ë³´ì„¸ìš”")
            suggestions.append("ì„œë²„ ë¶€í•˜ ìƒíƒœë¥¼ í™•ì¸í•˜ì„¸ìš”")

        if 'database' in message:
            suggestions.append("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„¤ì •ì„ í™•ì¸í•˜ì„¸ìš”")
            suggestions.append("ë””ìŠ¤í¬ ê³µê°„ì„ ì ê²€í•˜ì„¸ìš”")

        return suggestions or ["ë¡œê·¸ë¥¼ ìì„¸íˆ ë¶„ì„í•˜ì—¬ ì›ì¸ì„ íŒŒì•…í•˜ì„¸ìš”"]

    def _export_logs(self, format_type: str, range_type: str) -> str:
        """ë¡œê·¸ ë‚´ë³´ë‚´ê¸°"""
        # ë‚´ë³´ë‚¼ ë¡œê·¸ ì„ íƒ
        if range_type == "ì˜¤ë¥˜ë§Œ":
            logs_to_export = [log for log in self.log_cache if log.get('level') in ['ERROR', 'CRITICAL']]
        else:
            logs_to_export = self.log_cache

        if format_type == "CSV":
            import io
            output = io.StringIO()
            output.write("timestamp,level,source,message\n")
            for log in logs_to_export:
                output.write(f"{log.get('timestamp')},{log.get('level')},{log.get('source')},\"{log.get('message')}\"\n")
            return output.getvalue()

        elif format_type == "JSON":
            return json.dumps(logs_to_export, default=str, indent=2)

        else:  # TXT
            output = []
            for log in logs_to_export:
                output.append(f"{log.get('timestamp')} [{log.get('level')}] {log.get('source')}: {log.get('message')}")
            return "\n".join(output)

    def _cleanup_old_logs(self) -> int:
        """ì˜¤ë˜ëœ ë¡œê·¸ ì •ë¦¬"""
        cutoff = datetime.now() - timedelta(days=30)
        original_count = len(self.log_cache)
        self.log_cache = [log for log in self.log_cache if log.get('timestamp', datetime.now()) >= cutoff]
        return original_count - len(self.log_cache)

    def _recalculate_log_statistics(self):
        """ë¡œê·¸ í†µê³„ ì¬ê³„ì‚°"""
        # í†µê³„ ì¬ê³„ì‚° ë¡œì§
        pass


# ì „ì—­ í•¨ìˆ˜
def show_logs_viewer():
    """ë¡œê·¸ ë·°ì–´ í‘œì‹œ (ì™¸ë¶€ í˜¸ì¶œìš©)"""
    viewer = AdvancedLogsViewer()
    viewer.show_logs_viewer()

def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜"""
    show_logs_viewer()

if __name__ == "__main__":
    main()